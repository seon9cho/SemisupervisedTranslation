{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "SOS = 1\n",
    "EOS = 2\n",
    "UNK = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_path = \"dataset/europarl-v7.es-en.en\"\n",
    "es_path = \"dataset/europarl-v7.es-en.es\"\n",
    "with open(en_path, 'r', encoding='utf-8') as f:\n",
    "    en_str = f.read().strip().lower()\n",
    "with open(es_path, 'r', encoding='utf-8') as f:\n",
    "    es_str = f.read().strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lines = en_str.split('\\n')\n",
    "es_lines = es_str.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "_, en_small, _, es_small = train_test_split(en_lines, es_lines, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_small_train, en_small_test, es_small_train, es_small_test = train_test_split(en_small, es_small, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/europarl-v7small.es-en-train.en\", 'w', encoding='utf-8') as f:\n",
    "    for line in en_small_train:\n",
    "        f.write(line + '\\n')\n",
    "with open(\"dataset/europarl-v7small.es-en-train.es\", 'w', encoding='utf-8') as f:\n",
    "    for line in es_small_train:\n",
    "        f.write(line + '\\n')\n",
    "with open(\"dataset/europarl-v7small.es-en-test.en\", 'w', encoding='utf-8') as f:\n",
    "    for line in en_small_test:\n",
    "        f.write(line + '\\n')\n",
    "with open(\"dataset/europarl-v7small.es-en-test.es\", 'w', encoding='utf-8') as f:\n",
    "    for line in es_small_test:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnEsTranslationDataset():\n",
    "    def __init__(self, en_path, es_path, max_len=50):\n",
    "        self.max_len = max_len\n",
    "        self.en_lines, self.es_lines = self.preprocess_data(en_path, es_path)\n",
    "        self.en_vocab = None\n",
    "        self.en_word2index = None\n",
    "        self.en_index2word = None\n",
    "        self.es_vocab = None\n",
    "        self.es_word2index = None\n",
    "        self.es_index2word = None\n",
    "        self.en_data = None\n",
    "        self.es_data = None\n",
    "        \n",
    "    def preprocess_data(self, en_path, es_path):\n",
    "        print(\"Reading from file...\")\n",
    "        with open(en_path, 'r', encoding='utf-8') as f:\n",
    "            en_str = f.read().strip().lower()\n",
    "        with open(es_path, 'r', encoding='utf-8') as f:\n",
    "            es_str = f.read().strip().lower()\n",
    "            \n",
    "        print(\"Adding space between punctuations...\")\n",
    "        en_str = self.add_space_between_punctuation(en_str)\n",
    "        es_str = self.add_space_between_punctuation(es_str)\n",
    "        \n",
    "        print(\"Replacing numbers with <NUM> token...\")\n",
    "        en_str = self.remove_numbers(en_str)\n",
    "        es_str = self.remove_numbers(es_str)\n",
    "        \n",
    "        en_lines, es_lines = en_str.split('\\n'), es_str.split('\\n')\n",
    "        \n",
    "        print(\"Removing lines...\")\n",
    "        en_lines, es_lines = self.remove_lines(en_lines, es_lines)\n",
    "        \n",
    "        return en_lines, es_lines\n",
    "        \n",
    "    def add_space_between_punctuation(self, data_str):\n",
    "        '''Add space between a punctuation and word to tokenize punctuations separately'''\n",
    "        data_str = re.sub(r'(\\w)([\\.\\,\\!\\?\\:\\;\\'\\\"\\)\\(\\¡\\¿\\-])', r'\\1 \\2', data_str)\n",
    "        data_str = re.sub(r'([\\.\\,\\!\\?\\:\\;\\'\\\"\\)\\(\\¡\\¿\\-])(\\w)', r'\\1 \\2', data_str)\n",
    "        return data_str\n",
    "    \n",
    "    def remove_numbers(self, data_str):\n",
    "        '''replace all numbers to a <NUM> token'''\n",
    "        data_str = re.sub(r'\\b\\d+\\b', \"<NUM>\", data_str)\n",
    "        return data_str\n",
    "        \n",
    "    def remove_lines(self, l1, l2):\n",
    "        '''remove lines that are too short or dont end with .!?'''\n",
    "        indices = []\n",
    "        for i, line in enumerate(l1):\n",
    "            if len(line.split()) <= 2:\n",
    "                indices.append(i)\n",
    "            elif line[-1] not in set(\".!?\"):\n",
    "                indices.append(i)\n",
    "        for i, line in enumerate(l2):\n",
    "            if len(line.split()) <= 2 or len(line.split()) > self.max_len:\n",
    "                indices.append(i)\n",
    "            elif line[-1] not in set(\".!?\"):\n",
    "                indices.append(i)\n",
    "        indices = sorted(list(set(indices)), reverse=True)\n",
    "        for i in indices:\n",
    "            del l1[i]\n",
    "            del l2[i]\n",
    "        return l1, l2\n",
    "\n",
    "    def init_with_new_maps(self):\n",
    "        print(\"Generating vocab...\")\n",
    "        self.en_vocab = self.generate_vocab(self.en_lines)\n",
    "        self.es_vocab = self.generate_vocab(self.es_lines)\n",
    "        print(\"Generating maps...\")\n",
    "        self.en_word2index, self.en_index2word = self.generate_maps(self.en_vocab)\n",
    "        self.es_word2index, self.es_index2word = self.generate_maps(self.es_vocab)\n",
    "        print(\"Converting lines to indices...\")\n",
    "        self.en_data = self.convert_lines(self.en_lines, self.en_word2index)\n",
    "        self.es_data = self.convert_lines(self.es_lines, self.es_word2index)\n",
    "    \n",
    "    def init_using_existing_maps(self, en_vocab, en_word2index, en_index2word, es_vocab, es_word2index, es_index2word):\n",
    "        self.en_vocab = en_vocab\n",
    "        self.en_word2index = en_word2index\n",
    "        self.en_index2word = en_index2word\n",
    "        self.es_vocab = es_vocab\n",
    "        self.es_word2index = es_word2index\n",
    "        self.es_index2word = es_index2word\n",
    "        print(\"Converting lines to indices...\")\n",
    "        self.en_data = self.convert_lines(self.en_lines, self.en_word2index)\n",
    "        self.es_data = self.convert_lines(self.es_lines, self.es_word2index)\n",
    "    \n",
    "    def generate_vocab(self, data, min_freq=20):\n",
    "        vocab_cnt = Counter()\n",
    "        for line in data:\n",
    "            for word in line.split():\n",
    "                vocab_cnt[word] += 1\n",
    "        vocab = [word for word in vocab_cnt if vocab_cnt[word] > min_freq]\n",
    "        return vocab\n",
    "    \n",
    "    def generate_maps(self, vocab):\n",
    "        word2index = {'PAD':PAD, 'SOS':SOS, 'EOS':EOS, 'UNK':UNK}\n",
    "        index2word = {PAD:'PAD', SOS:'SOS', EOS:'EOS', UNK:'UNK'}\n",
    "        for i, w in enumerate(vocab):\n",
    "            word2index[w] = i + 4\n",
    "            index2word[i+4] = w\n",
    "        return word2index, index2word\n",
    "    \n",
    "    def sentence2index(self, s, _map):\n",
    "        out = []\n",
    "        for w in s.split():\n",
    "            if w in _map:\n",
    "                out.append(_map[w])\n",
    "            else:\n",
    "                out.append(_map['UNK'])\n",
    "        return out\n",
    "    \n",
    "    def convert_lines(self, lines, _map):\n",
    "        out = []\n",
    "        for line in lines:\n",
    "            out.append(self.sentence2index(line, _map))\n",
    "        return out\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.en_data[idx], self.es_data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.en_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def padding_collate_fn(batch):\n",
    "    batch_size = len(batch)\n",
    "    # Get the max length of an input sequence (each item is an input seq and a label)\n",
    "    max_length1 = max([len(item[1]) for item in batch])\n",
    "    max_length2 = max([len(item[0]) for item in batch]) + 1\n",
    "    # Its weird but the first item is a tuple not a tensor. \n",
    "    # print('data',[item[0][0].shape for item in batch])\n",
    "    src = torch.zeros((batch_size, max_length1), dtype=torch.float)\n",
    "    trg = torch.zeros((batch_size, max_length2), dtype=torch.float)\n",
    "    trg_y = torch.zeros((batch_size, max_length2), dtype=torch.float)\n",
    "    \n",
    "    for i, seq in enumerate(batch):\n",
    "        trg_seq = seq[0].copy()\n",
    "        trg_seq.insert(0, SOS)\n",
    "        trg_y_seq = seq[0].copy()\n",
    "        trg_y_seq.append(EOS)\n",
    "        src[i, :len(seq[1])] = torch.tensor(seq[1])\n",
    "        trg[i, :len(seq[0])+1] = torch.tensor(trg_seq)\n",
    "        trg_y[i, :len(seq[0])+1] = torch.tensor(trg_y_seq)\n",
    "    \n",
    "    src = src.long()\n",
    "    trg = trg.long()\n",
    "    trg_y = trg_y.long()\n",
    "    \n",
    "    src_key_padding_mask = (src == PAD)\n",
    "    trg_key_padding_mask = (trg == PAD)\n",
    "    \n",
    "    return src, trg, trg_y, src_key_padding_mask, trg_key_padding_mask\n",
    "\n",
    "def index2sentence(ind, _map):\n",
    "    out = []\n",
    "    for i in ind:\n",
    "        if i == PAD: continue;\n",
    "        out.append(_map[i])\n",
    "    return ' '.join(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab1, vocab2, d_model=256, nhead=8, num_layers=6, dim_feedforward=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding1 = nn.Embedding(vocab1, d_model)\n",
    "        self.embedding2 = nn.Embedding(vocab2, d_model)\n",
    "        self.pe = PositionalEncoding(d_model, dropout)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            num_encoder_layers=num_layers, \n",
    "            num_decoder_layers=num_layers, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.generator = Generator(d_model, vocab2)\n",
    "    \n",
    "    def forward(self, src, trg, trg_mask, skpm=None, tkpm=None):\n",
    "        src_emb = self.embedding1(src).permute(1, 0, 2) * math.sqrt(self.d_model)\n",
    "        src_emb = self.pe(src_emb)\n",
    "        trg_emb = self.embedding2(trg).permute(1, 0, 2) * math.sqrt(self.d_model)\n",
    "        trg_emb = self.pe(trg_emb)\n",
    "        out = self.transformer(\n",
    "            src_emb, trg_emb, \n",
    "            tgt_mask=trg_mask, \n",
    "            src_key_padding_mask=skpm, \n",
    "            tgt_key_padding_mask=tkpm, \n",
    "            memory_key_padding_mask=skpm\n",
    "        )\n",
    "        return self.generator(out)\n",
    "    \n",
    "    def greedy_decode(self, src, max_len=50):\n",
    "        src = torch.LongTensor([i for i in src if i != 0]).to(device)\n",
    "        src = src.unsqueeze(0)\n",
    "        src_emb = self.embedding1(src).permute(1, 0, 2)\n",
    "        memory = self.transformer.encoder(src_emb)\n",
    "        ys = torch.ones(1, 1).fill_(SOS).type_as(src.data).to(device)\n",
    "        for i in range(max_len-1):\n",
    "            ys = Variable(ys)\n",
    "            ys_emb = self.embedding2(ys).permute(1, 0, 2)\n",
    "            out = self.transformer.decoder(ys_emb, memory)\n",
    "            prob = self.generator(out[:, -1])\n",
    "            _, next_word = torch.max(prob, dim = 1)\n",
    "            next_word = next_word.data[0]\n",
    "            ys = torch.cat([ys, \n",
    "                            torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "            if next_word.item() == EOS:\n",
    "                break\n",
    "        return ys[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "def get_std_opt(model, warmup):\n",
    "    return NoamOpt(model.d_model, 2, warmup,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
    "\n",
    "def ComputeLoss(x, y, norm, criterion):\n",
    "    reconstruction_loss = criterion(x.contiguous().view(-1, x.size(-1)), \n",
    "                                    y.contiguous().view(-1)) / norm\n",
    "    loss = reconstruction_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def run_instance(batch, model, criterion, optimizer):\n",
    "    src, trg, trg_y, skpm, tkpm = batch\n",
    "    ntokens = (trg_y != PAD).data.sum()\n",
    "    trg_mask = model.transformer.generate_square_subsequent_mask(trg.size(1))\n",
    "    src, trg, trg_y, trg_mask, skpm, tkpm = src.to(device), trg.to(device), trg_y.to(device), trg_mask.to(device), skpm.to(device), tkpm.to(device)\n",
    "    \n",
    "    output = model(src, trg, trg_mask)\n",
    "    loss = ComputeLoss(output, trg_y, ntokens, criterion)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return loss\n",
    "\n",
    "def train(dataloader, model, criterion, optimizer, index2word1, index2word2, num_epochs, print_every=100):\n",
    "    history = []\n",
    "    start = time.time()\n",
    "    for e in range(num_epochs):\n",
    "        temp_history = []\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            loss = run_instance(batch, model, criterion, optimizer)\n",
    "            temp_history.append(loss.item())\n",
    "            if i % print_every == 0:                \n",
    "                model.eval()\n",
    "                decoded = model.greedy_decode(batch[0][0])\n",
    "                model.train()\n",
    "                checkpoint = time.time()\n",
    "                history.append(np.mean(temp_history))\n",
    "                elapsed = checkpoint - start\n",
    "                remaining = (elapsed / (i+1)) * (len(dataloader) - (i+1))\n",
    "                print_str1 = \"Epoch: {}, Iteration: {}, loss: {:.4f}, elapsed: {:.2f}, remaining: {:.2f}\"\\\n",
    "                             .format(e, i, np.mean(temp_history), elapsed, remaining)\n",
    "                l1 = (batch[0][0] != 0).sum().item()\n",
    "                print_str2 = \"Input: \" + index2sentence(batch[0][0][:l1].tolist(), index2word1)\n",
    "                print_str3 = \"Output: \" + index2sentence(decoded[1:].tolist(), index2word2)\n",
    "                l2 = (batch[2][0] != 0).sum().item()\n",
    "                print_str4 = \"Target: \" + index2sentence(batch[2][0][:l2].tolist(), index2word2)\n",
    "                temp_history = []\n",
    "                \n",
    "                print(print_str1)\n",
    "                print(print_str2)\n",
    "                print(print_str3)\n",
    "                print(print_str4)\n",
    "                print()\n",
    "                \n",
    "            del batch\n",
    "            gc.collect()\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from file...\n",
      "Adding space between punctuations...\n",
      "Replacing numbers with <NUM> token...\n",
      "Removing lines...\n",
      "Generating vocab...\n",
      "Generating maps...\n",
      "Converting lines to indices...\n"
     ]
    }
   ],
   "source": [
    "en_train_path = \"dataset/europarl-v7small.es-en-train.en\"\n",
    "es_train_path = \"dataset/europarl-v7small.es-en-train.es\"\n",
    "train_dataset = EnEsTranslationDataset(en_train_path, es_train_path)\n",
    "train_dataset.init_with_new_maps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 118460\n",
      "EN vocab size:  5887\n",
      "ES vocab size:  7035\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of lines:\", len(train_dataset))\n",
    "print(\"EN vocab size: \", len(train_dataset.en_vocab))\n",
    "print(\"ES vocab size: \", len(train_dataset.es_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "num_layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    "    collate_fn=padding_collate_fn,\n",
    "    shuffle=True\n",
    ")\n",
    "vocab1_size = len(train_dataset.es_word2index)\n",
    "vocab2_size = len(train_dataset.en_word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(vocab1_size, vocab2_size, num_layers=num_layers).to(device)\n",
    "criterion = LabelSmoothing(size=vocab2_size, padding_idx=PAD, smoothing=0.1)\n",
    "optimizer = get_std_opt(model, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 0, loss: 7.6250, elapsed: 0.83, remaining: 1537.87\n",
      "Input: pero la responsabilidad final de la coordinación recae en la osce .\n",
      "Output: , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "Target: but the ultimate responsibility for coordination rests with the osce . EOS\n",
      "\n",
      "Epoch: 0, Iteration: 100, loss: 5.5542, elapsed: 41.07, remaining: 711.63\n",
      "Input: ¿ no deberían UNK estas UNK específicas en un momento en el que la unión está trabajando en un sistema de financiación que se centra en el mercado interior ?\n",
      "Output: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Target: should these specific UNK not be consolidated at a time when the union is working on a funding system focused on the internal market ? EOS\n",
      "\n",
      "Epoch: 0, Iteration: 200, loss: 5.3002, elapsed: 81.63, remaining: 670.13\n",
      "Input: acojo con satisfacción la política UNK por el parlamento europeo , la comisión y la presidencia británica en relación con este asunto .\n",
      "Output: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Target: i welcome the policy set out by the european parliament , the commission and the british presidency regarding this issue . EOS\n",
      "\n",
      "Epoch: 0, Iteration: 300, loss: 5.2540, elapsed: 121.74, remaining: 626.91\n",
      "Input: no nos debemos UNK con apoyar a unas pocas medianas empresas y UNK de que las UNK y las pequeñas empresas desempeñan un papel vital en la actividad económica y en el empleo de trabajadores , cuyos derechos también deben UNK con este proceso .\n",
      "Output: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Target: we must not be satisfied with just supporting a few medium - sized enterprises and UNK about the micro - and small enterprises that play a vital role in economic activity and the employment of workers , whose rights must also be guaranteed throughout this process . EOS\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-7b90139705e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mes_index2word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0men_index2word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-52-b6fa8bfbbf6c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, criterion, optimizer, index2word1, index2word2, num_epochs, print_every)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mtemp_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mtemp_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-af021cd04e08>\u001b[0m in \u001b[0;36mpadding_collate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Its weird but the first item is a tuple not a tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# print('data',[item[0][0].shape for item in batch])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mtrg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtrg_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_loader, model, criterion, optimizer, train_dataset.es_index2word, train_dataset.en_index2word, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
